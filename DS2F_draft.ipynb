{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "213a3be7-bddb-4a47-b580-86b3bfeae8fb",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-info\">\n",
    "<h1>DS2F Workshop</h1>\n",
    "\n",
    "<h2>Coding and Decoding: Introduction to Text Analytics for Humanists and Social Scientists</h2>\n",
    "    <h3> by Anuj Gupta </h3>\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7fda77-89c6-48ec-993c-2f384cfbf704",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-info\">\n",
    "<h3> INTRODUCTION </h3>\n",
    "<h4>Who am i? </h4>\n",
    "Anuj Gupta is a University Fellow and a Ph.D. student in the Rhetoric, Composition and the Teaching of English program in the Department of English. He is interested in UX research, emotions, text analytics, diversity, and accessibility. He brings these interests together by building ed tech tools to help students and teachers to read, write, think, and learn better. You can contact him by email (anujgupta@arizona.edu) or connect with him on twitter and Linkdin. \n",
    "\n",
    "\n",
    "\n",
    "<h4>What is this workshop about?</h4>\n",
    "Like what the microscope did for biology, and what the telescope did for astronomy, text mining is helping humanists and social scientists explore their data with fresh, new perspectives. <b>Text mining</b> is basically the use of computer languages like Python or R to find patterns in large textual datasets and it can be a really powerful tool to unpack socio-cultural patterns. Participants in this workshop will learn how to add text mining tools to their arsenal of research methods to explore their research interests.\n",
    "    \n",
    "<h4> What will you achieve by completing this workshop? </h4>\n",
    "\n",
    "By completing this workshop, you will: \n",
    "\n",
    "<b> Goal 1:</b> EXPLORE iconic studies in Humanities and Social Sciences that have used text mining techniques\n",
    "\n",
    "\n",
    "<b> Goal 2: </b>PRACTICE how to use computational methods like frequency distributions, collocations, sentiment analysis and data visualizations on sample textual datasets\n",
    "\n",
    "\n",
    "<b> Goal 3: </b>BRAINSTORM how some of those techniques could be applied to participants’ own work\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39467d6d-c58c-4ad1-b8e7-a46f88de4ec3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "***Okay so let's get started!***\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbb4e14-5363-4361-a72b-f154706954f1",
   "metadata": {},
   "source": [
    "<h4> First things first. What exactly is text mining and why should we care?</h4>\n",
    "\n",
    "1) “Of course computers should be used by scholars in the humanities, just as microscopes should be used by scientists. . . [t]he facts and patterns that they—and often they alone—can reveal should be viewed not as the definitive answers to the questions that humanists have been asking, but rather as the occasion for a whole range of new and more penetrating and more exciting questions” (ACLS, 2006).\n",
    "\n",
    "2) Simply put, “text mining” or “text analytics” or “computational text analysis” or “corpus linguistics” are allied terms that refer to the use of computer languages like Python to find interesting patterns in large textual datasets that help researchers solve real world problems and explore questions that they are interested in. \n",
    "\n",
    "3) While text mining “cannot reproduce the subtlety of a creative researcher who brings a life of prior associations to their analysis…[but] computational methods trained on big data can generate many suggestive, subtle associations beyond the sensitivity of human perception and the capacity of human memory” (Evans and Aceves, 2016)\n",
    "\n",
    "4) The difference between human text reading and computerised text mining is: a) Scale: It exponentially increases how many texts can be read and drastically reduces the time required to do so. b) Patterns: It exponentially increases the kinds of patterns between the words and sentences that can be found, many of which are usually insivible to the naked eye.  \n",
    "\n",
    "5) “In summary, although computation cannot mimic the prior experience, vision, and unexpected associations of a gifted analyst, it can augment their reliability and provide new data—regularities, associations, and structures built from much larger text samples—which sociology can mine to deepen and expand our inferences about the social games and worlds underlying communication.” (Evans and Avces 2016:25)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930e0514-7851-43db-817a-85d8967d1b6b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "***Before jumping into doing some text mining, let's understand this strange notebook called Jupyter Notebook that we are using***\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24565a3-3c75-4828-a9df-cc75224a86b0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <h1> Part 1: Basic Setup </h1>\n",
    "\n",
    "<p>\n",
    "\n",
    "This is something called a Jupyter Notebook. Think of this as a Google Docs for text mining. \n",
    "Here you can write computer code and run it. It's all pretty simple once you get the hang of it.  \n",
    "    \n",
    "Here is everything you need to use it: \n",
    "    <div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c484cb76-bf75-41a8-acd4-c447d4b13156",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<h5> 1. Let's familiarize ourselves with the toolbar. You should be something similar to the image below at the top right corner of your screen </h5>\n",
    "\n",
    "</p>\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f381d34f-6683-48f3-8175-cb24c2fbd9d3",
   "metadata": {},
   "source": [
    "![](Images/jupytertoolbar.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc71ff6d-792b-481c-a0ab-2f7e6e4bc0fe",
   "metadata": {},
   "source": [
    "<p> The table below shows what each button does </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7442564a-9230-4af8-bb32-959b2022d9ff",
   "metadata": {},
   "source": [
    "\n",
    "Button|Function\n",
    "-|-\n",
    "![](Images/jupytertoolbarsave.png)|This is your save button. You can click this button to save your notebook at any time, though keep in mind that Jupyter Notebooks automatically save your progress very frequently.  \n",
    "![](Images/jupytertoolbarnewcell.png)|This is the new cell button. You can click this button any time you want a new cell in your Jupyter Notebook. \n",
    "![](Images/jupytertoolbarcutcell.png)|This is the cut cell button. If you click this button, the cell you currently have selected will be deleted from your Notebook. \n",
    "![](Images/jupytertoolbarcopycell.png)|This is the copy cell button. If you click this button, the currently selected cell will be duplicated and stored in your clipboard. \n",
    "![](Images/jupytertoolbarpastecell.png)|This is the past button. It allows you to paste the duplicated cell from your clipboard into your notebook. \n",
    "![](Images/jupytertoolbarupdown.png)|These buttons allow you to move the location of a selected cell within a Notebook. Simply select the cell you wish to move and click either the up or down button until the cell is in the location you want it to be.\n",
    "![](Images/jupytertoolbarrun.png)|This button will \"run\" your cell, meaning that it will interpret your input and render the output in a way that depends on [what kind of cell] [cell kind] you're using. \n",
    "![](Images/jupytertoolbarstop.png)|This is the stop button. Clicking this button will stop your cell from continuing to run. This tool can be useful if you are trying to execute more complicated code, which can sometimes take a while, and you want to edit the cell before waiting for it to finish rendering. \n",
    "![](Images/jupytertoolbarrestartkernel.png)|This is the restart kernel button. See your kernel documentation for more information.\n",
    "![](Images/jupytertoolbarcellkind.png)|This is a drop down menu which allows you to tell your Notebook how you want it to interpret any given cell. You can read more about the [different kinds of cells] [cell kind] in the following section. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb11dba-25fd-4cc0-b8b4-8fb60b0ec14e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "       \n",
    "<h5> 2. We need to learn how to run cells. A cell is simply a space where you can write or edit code. Try to run the cell below! Just click on the cell and press the ▶ button above to run it.</h5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb0f28a-b14e-4844-b8fb-5155467818f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hi! This is a cell. Click on this cell and press the ▶ button above to run it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369cfe4e-65b0-48cc-9224-552e10b01114",
   "metadata": {},
   "source": [
    "Notice, that the input code cell, which was once represented as: \n",
    "\n",
    "`In [ ]:` \n",
    "\n",
    "has now become: \n",
    "\n",
    "`In [1]:` \n",
    "\n",
    "This means your code in that particular cell has run, and it was the first line of code that ran (hence the `1`). This number could vary from notebook to notebook depending on how many cells you have run before this. \n",
    "\n",
    "***\n",
    "\n",
    "This code ran quickly. However, sometimes it might take some time. If your code is still running it'll look like this:\n",
    "\n",
    "`In [*]:`\n",
    "\n",
    "That \"star\" in between the brackets means this cell of code is still running. Be on the lookout whenever you run your code in Jupyter Notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb3cb36-bc92-4e76-9bff-1f7664e54fd7",
   "metadata": {
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "       \n",
    "<h5> 3. Finally, let's try and understand in very simple terms what Python is and how it works.</h5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba58a06-7e22-4b22-96c0-0be44609195c",
   "metadata": {},
   "source": [
    "<p> Python is one of the many computer languages that exist in the world. It is used widely in text mining because compared to  many of the other languages out there, it is relatively the closed to English language and easiest to learn. I love it because it is:\n",
    "    \n",
    "    \n",
    "    1. Easy to use\n",
    "    2. Lots of help available online. \n",
    "    3. You can easily copy paste other people's code and customize it for your research. \n",
    "\n",
    "In this tutorial we obviously don't have time to go into the intracies of how Python works but we will get a very basic surface level understanding of it. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6917edde-2741-4fed-967f-7b86f8d01c8e",
   "metadata": {},
   "source": [
    "<p> When we write code in Python, we are simply writing commands in a language that our computer understands. Essentially we are telling the computer to do something for us. For example, let's use Python to tell the Computer to do some addition for us:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f9d6fc-f401-47fa-945c-866f86ef958a",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_number = 5\n",
    "second_number = 7 \n",
    "sum = first_number + second_number \n",
    "print(\"the sum of these numbers is \" +str(sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e938563-9fa3-4e0c-9533-c1b72338aed8",
   "metadata": {},
   "source": [
    "<p>Excellent! You just ran your first Python code! What we did here is simply to create variables called \"first_number\" and \"second_number\" and then to store certain data in them (the numbers 5 and 7). Then we created a new variable called \"sum\" and told the computer that we want to store the sum of those first two variables in that third variable. Finally, we asked the computer to print or display whatever is stored in that third variable called \"sum\" along with some text \"the sum of these numbers is \" in the display space. That's it! That's all there is to computer programming! If you understand this basic syntax, you can do amazing and powerful things. Everything you use on a computer, including social media, netflix, games etc. everything has been built using these rudimentary building blocks. Its all about building more and more complexity to this basic structure. Amazing isn't it? </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e2670f-73b6-44a3-82fc-4d5a1549b5a3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "       \n",
    "<h5> 4. Now that you understand how a basic Python code is written, let's add some complexity by learning what a \"library\" and a \"function\" is.</h5>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7309039-de44-47fa-95c6-fd7891ef4b90",
   "metadata": {},
   "source": [
    "<h5> 4.1 Understanding functions in Python </h5>\n",
    "<p> The really cool thing about Python is that instead of always writing fresh code, we can recycle code that others have written, almost like how in academic writing we use ideas of other scholars to build up our own work. Each bit of code written by someone else that can use is called a \"function\". For example, in the above cells we just wrote a code for a \"sum\" function that could add two numbers together. \n",
    "    \n",
    "    Function = collection of code that performs a particular purpose. \n",
    "    \n",
    "Let's what this looks like in practice.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0cff8f-f6dd-4cde-af02-ae156e282f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"i am a human being trying to learn text mining\" \n",
    "count =len(text)\n",
    "print(\"the total number of characters in my text is \" + str(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeaa6111-50aa-4346-941d-7bb87da34da9",
   "metadata": {},
   "source": [
    "<p>Here we have used a function called \"len\" which means \"length\" and it basically just calculates the number of characters (alphabets, numbers, punctuation, spaces etc.) that we have stored in a variable </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68022347-6c68-43b5-a28a-182869b2d756",
   "metadata": {},
   "source": [
    "<p> Just like this \"len\" function there are millions of functions for which people across the world have written code so that we don't need to reinvetent the wheel each time. We can simply recycle these functions for our purposes and stand on the shoulders of gians, kind of like how in academic writing we use ideas from other scholars.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ff6791-df67-45fd-8ef8-774706f33cbc",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "<div class = \"alert alert-info\">\n",
    "    \n",
    "<b>Now you try!</b>\n",
    "\n",
    "In the cells below, first replace the text that says \"input text here\" with any sentence you like. Then click on the cell that follows to see how many characters it contains\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61738c78-a418-454b-aaf5-e7816b164f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_text = \"input your text here\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf398747-bd14-4ed4-8bbe-51916e02cbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_count =len(my_text)\n",
    "print(\"the total number of characters in my text is \" + str(my_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a88fba-2674-4fee-b05d-7958d98372bd",
   "metadata": {},
   "source": [
    "<h5> 4.2: Understanding libraries in Python </h5>\n",
    "\n",
    "<p> Now if people simply wrote individual functions and uploaded them it could create chaos because there would be billions of functions just floating around in the world, make it very difficult for us to see which functions are useful for us to learn. So to create some method in the madness, people have come up with the idea of \"libraries\" which are simply collections of functions clubbed together based on how connected they are.  \n",
    "    \n",
    "    Library = collection of functions that perform similar roles.  \n",
    "    \n",
    "Think of functions like a chapter and a library like a book that holds them together</p>\n",
    "\n",
    "Let's see what this looks like in practice "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bff814-ec36-4d3c-a9ee-384bcbfdf3ab",
   "metadata": {},
   "source": [
    "<p> First let us download a library called \"nltk\" or Natural Language ToolKit (© 2022, NLTK Project) which is a specific library created by these amazing folks (https://www.nltk.org/team.html) to help us do text mining.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bf5d39-c757-4b0e-8ee2-4a481bc63184",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"book\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c336ff8-4ab7-4a3c-ba0c-2168ccca2c01",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "Once you click on the cell above, a pop up will appear. Click on \"book\" there and then click \"install\". Once its installed, close the close button and return to the notebook. \n",
    "    \n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fd4539-31ca-49f3-9dad-142a6de9b17c",
   "metadata": {},
   "source": [
    "<h5> 4.3 Exploring the NLTK Library </h5>\n",
    "\n",
    "<p> The NLTK or Natural Language Tool Kit Library © 2022, NLTK Project is a library that the wonderful folks in this team (https://www.nltk.org/team.html)created to help folks like us do some cool text mining!\n",
    "\n",
    "\n",
    "It essentially contains a whole bunch of corpora (collecting of famous texts) and text mining functions that we can use to expore them</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fc1bd3-f659-4d68-bd08-8247394652f9",
   "metadata": {},
   "source": [
    "<h5> 4.4 Loading corpora </h5>\n",
    "<p> A corpus is simply a collection of texts and corpora is plural for corpus. The NLTK library, apart from containing amazing text mining functions also contains some really cool in-built corpora that we can use use to have some fun. Let's download a few them by clicking on the cell below.\n",
    "\n",
    "As always, don't worry about the specific coding syntax we are using to do this as this workshop is not so much about learning the particular syntax but about gaining familiarity what what it can dox</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b413bbd-3b1f-4abe-8714-c19d980032bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.book import*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcdb5b9-4bf2-4e5b-a4cb-25771b7a031f",
   "metadata": {},
   "source": [
    "<p> As we can see here, Python has just downloaded 9 corpora for each and stored them in variables ranging from text1...text9. Each of these is either a famous literary work (like text1 --> Moby Dick) or collection of important socio-political texts (like text4 --> a collection of the inaugural addresses given by all american presidents till today) or a smaller subsection of web-based corpora stored in the NLTK module (like text5 --> Chat Corpus is a collection of 10,567 posts collected from various online chat services; and text8 --> this is a collection of personal advertisements in newspapers </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204e8cfe-a5ab-4d5e-8311-54f9ab3f9423",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h5> 4.5 Using NLTK Functions </h5>\n",
    "\n",
    "<p> By now, I hope that you are familiar with what functions are. The NLTK has thousands of functions which help us do different kinds of text mining. Let's see how one of these functions called \"concordance\". It is a function that allows us to see the context in which key words that interest appear in the corpora of our choice</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc297fac-77dd-4434-9498-39979e14ada6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.concordance(\"God\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3686aa-fba1-488b-8481-f8ac5ce2dc07",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <h1> Part 2: Text Mining Techniques </h1>\n",
    "\n",
    "<p>\n",
    "\n",
    "Now that we understand the basics, let's start exploring some cool text mining techniques. \n",
    "    \n",
    "Broadly speaking, we can classify most existing text-mining techniques into two categories: \n",
    "    \n",
    "<b> 2.1: Distant Reading (macro-analysis): </b> distant reading \"aims to generate an abstract view by shifting from observing textual content to visualizing global features of a single or of multiple text(s)\" (Janicke et al. 2015)\n",
    "    \n",
    "<b> 2.2: Close reading (micro-analysis):  </b> \"close reading is the thorough interpretation of a text passage by the determination of central themes and the analysis of their development. Moreover, close reading includes the analysis of (1) individuals, events, and ideas, their development and interaction, (2) used words and phrases, (3) text structure and style, and (4) argument patterns\" (Janicke et al. 2015) \n",
    "    \n",
    "<b> Importance of combining the two approaches </b>: \"distant reading visualizations cannot replace close reading, but they can direct the reader to sections that may deserve further investigation\" (Janicke et al. 2015) \n",
    "    \n",
    "We will now practice several techniques in each of these categories.\n",
    "    \n",
    "Please keep in mind that you are not expected to memorize or fully comprehend the specific syntax of the code that we will now use. Rather, you are expected to get a sense of its broad functionality and learn how to customise it for your purposes \n",
    "\n",
    "</p>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dee5cd9-4d13-426a-8e17-704804fcd4f5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "       \n",
    "<h3> 2.1 Technique Type: 1: Distant Reading (macro-analysis)</h3>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81f56e1-9adb-4058-ae49-b514449a1282",
   "metadata": {},
   "source": [
    "***\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "       \n",
    "<h4> 2.1.1: Raw Frequencies</h4>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31e0590-4878-4112-8f22-d9565342b165",
   "metadata": {},
   "source": [
    "<h5> 2.1.1.1: What is \"frequency\" and why should we care? </h5>\n",
    "\n",
    "<p> Counting the frequency of words in a text or corpus is the most basic text mining function that can be performed.“High-frequency words are valuable because they have “aboutness”; they suggest what the overall textual object is about (Archer, 2009a, p. 4). The frequency of words is “a relatively objective means of uncovering lexical salience/(frequency) patterns that invite—and frequently repay—further qualitative investigation,” as Dawn Archer (2009a, p. 15) states” (Carradini 43 qtd. In Schreiber and Melonçon, 2021). \n",
    "    \n",
    "So counting is the basic building block. Another building block is classification -- POS taggers or other things. \n",
    "    Based on these two basic functions (identification + counting), a whole range of things can be done. For example, calculating readability difficulty scores -- create a function to show this. or if you are interested in linguistic analysis POS taggers, or finding collocates..or sentiment analysis. \n",
    "\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdaffb3-3e52-41bc-972d-71e34662cac7",
   "metadata": {},
   "source": [
    "<b>Coding practice</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e553254e-e115-45a9-8bd7-63a8eaa80a83",
   "metadata": {},
   "source": [
    "We can use a simple function called \"count\" to find the frequency or the number of times a word appears in a text. Let's see how it works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77f7f4e-0113-4bd8-8571-3e9bc8e10484",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.count(\"God\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454367d3-68b7-4864-9b2e-60bd467beb2a",
   "metadata": {},
   "source": [
    "This shows us that the word \"God\" appears 132 times in the book Moby Dick. How many times do you think it would appear in the Book of Genesis? More or less than in Moby Dick? Let's find out! Why don't you try to write some code now to find the answer to that question. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7f8ba0-6807-4eb7-a64d-f35e906caeee",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-info\">\n",
    "    \n",
    "<b> Now you try! </b>\n",
    "    \n",
    "Write code in the box below to find out how many times the word \"God\" appears in the Book of Genesis. Remember that in the corpus we downloaded, the Book of Genesis is called text3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef6d7a7-b718-453e-bfd2-13df73ffd9ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "82f0c2f9-213d-42c0-8fba-3f75c4c6db19",
   "metadata": {},
   "source": [
    "Excellent! Now this simple technique of counting frequency of different words can lead to many interesting insights. \n",
    "\n",
    "Things start to get interesting when we compare such frequencies across texts. Let's see how the frequency of the word \"God\" varies across the texts that we have. \n",
    "\n",
    "In the code below, we are creating 9 new variables named a,b,c,d,e,f,g,h,i and in each of them we are storing the total count of the word \"God\" in each of our 9 texts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b3ad8a-9c42-48e6-bf04-bbee93212597",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = text1.count(\"God\")\n",
    "b = text2.count(\"God\")\n",
    "c = text3.count(\"God\")\n",
    "d = text4.count(\"God\")\n",
    "e = text5.count(\"God\")\n",
    "f = text6.count(\"God\")\n",
    "g = text7.count(\"God\")\n",
    "h = text8.count(\"God\")\n",
    "i = text9.count(\"God\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f5690c-541e-455c-a287-ebd9dfc5dbc4",
   "metadata": {},
   "source": [
    "Now we are simply printing or displaying these counts using the \"print\" function that we learnt about earlier. \n",
    "\n",
    "Be patient. All of this might seem rudimentary but once it starts to add up, amazing insights come together :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49eae968-83ed-4aae-adea-1b3cc22f6a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Frequency Distribution of the word God in the following texts:\")\n",
    "print(\"Moby Dick = \" + str(a))\n",
    "print(\"Sense and Sensibility = \" + str(b))\n",
    "print(\"The Book of Genesis = \" + str(c))\n",
    "print(\"Inaugural Address Corpus =\" + str(d))\n",
    "print(\"Chat Corpus = \" + str(e))\n",
    "print(\"Monty Python and the Holy Grail = \" + str(f))\n",
    "print(\"Wall Street Journal = \" + str(g))\n",
    "print(\"Personal Corpus = \" + str(h))\n",
    "print(\"The Man Who Was Thursaday = \" + str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c807cc-bfe6-4527-891e-15454be5061a",
   "metadata": {},
   "source": [
    "<h5> 2.1.1.2 Visualizing raw frequencies using bar graphs </h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e48e7d-9bed-47db-82c3-4d38c0753be7",
   "metadata": {},
   "source": [
    "For some people, reading data like this can seem a little confusing. This is where data visualization can help! Let's try to create a bargraph that will make it easier for us to compare these values. To do this though, we need to install a few extra packages in Python, so let's quickly do that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f07e039-09b1-4ab9-857d-f448e75f19c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d0e061-4b62-481b-b173-4ea6d721419c",
   "metadata": {},
   "source": [
    "Now we need to use code that tells the computer to create a bargraph using the data we have. It's not necessary that you understand the intracacies of this code. A simple Google search \"how to create bargraphs in python using matplotlib\" gives you the code below and then you can simply customise it to fit your data. What I mean is that the code that I code from the internet for this had different data and I just replaced that with the names of the texts and frequency of the word \"God\" in them that we have calculated. This is a key skill that we need to master over time -- how to customise existing Python code on the internet for our research purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb93ad88-ce40-431a-b192-4a9799de3f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "texts = ['Moby', 'Sense', 'Genesis', 'Inaugural', 'Chat','Monty','Wall','Personal','Man']\n",
    "frequency = [a,b,c,d,e,f,g,h,i]\n",
    "ax.bar(texts,frequency)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d34a3d-18d4-4354-9fec-1440d9e0b1da",
   "metadata": {},
   "source": [
    "So we see here that the word \"God\" appears most frequently in the Book of Genesis, which is well, obvious, because this is a religious book. However, interestingly we see that it appears quite frequently also in a book called Moby Dick and the Inaugural Address speeches of American Presidents. But it is quite infrequent in corpora that represent personal writing and web-based chats. \n",
    "\n",
    "<h5> 2.1.1.3 What kind of inferences could we draw from this? </h5>\n",
    "\n",
    "<b> Hint </b> There can be many inferences that one might draw about what the different frequencies of the word \"God\" reveals to us about these texts and the cultures that they come from. Try to think of categories using which we can diffrentiate between texts in our corpus. What happens when we compare them based on their genre? What happens when we compare them based on their years of publication? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c70b58-ca31-4398-a7a0-1411473451ef",
   "metadata": {},
   "source": [
    "***\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "       \n",
    "<h4> 2.1.2: Normalized Frequencies</h4>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d680b1-5473-4ff7-a244-5a1082026b38",
   "metadata": {},
   "source": [
    "<h5> 2.1.2.1: What is normalized freqency and why does it matter? </h5>\n",
    "\n",
    "Fun right? While this might seem quite revealing, to make our inferences more robust, we need to ensure that our data is valid. Right now, we have counted pure frequencies of words but in order to compare them across texts, we need to have normalized frequencies. Image if the word \"God\" appears 10 times in textA and 5 times in textB. On the surface level it would make us infer that the word \"God\" is more important in textA right? However, what if textA contains 10,000 words and textB contains only 100 words. That would make the word \"God\" more relevant in textB right? This is why we need to calculate normalized frequencies per million words which can be done simply using this formula:\n",
    "\n",
    "<b>Normalized frequency of word w in text T = (number of times word w appears in text T/ total number of words in text T)* 1000000 </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554476ff-c59d-4999-981b-84f1e34a60b7",
   "metadata": {},
   "source": [
    "To do this, we need to first calculate the total number of words in each of our texts using the \"len\" function we learnt earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7167769d-543e-4061-8361-7c8a37ea5331",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76948d81-61d7-4772-995b-95355d7f09fe",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-info\">\n",
    "    \n",
    "Now you try. \n",
    "\n",
    "Try to find out total number of units in text2 in the box below. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3214d278-c069-4afa-9f0f-6e2fc36ed502",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee6d3278-44fd-4f28-9173-8cbafac38361",
   "metadata": {},
   "source": [
    "Good job! Now using the code below let's find out the overall lengths of all the texts in our collection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84b7f26-3663-4588-bfb6-92dfab7e73ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_len=len(text1) \n",
    "b_len=len(text2) \n",
    "c_len=len(text3) \n",
    "d_len=len(text4) \n",
    "e_len=len(text5) \n",
    "f_len=len(text6)\n",
    "g_len=len(text7) \n",
    "h_len=len(text8) \n",
    "i_len=len(text9) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d54fea-2d7f-4410-abf7-94c71cd20c2e",
   "metadata": {},
   "source": [
    "Now let's print these results in a way we can read them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad35b6d-f301-4f3c-8cba-88f60ba8ed70",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The length of words (including punctuations) in Moby Dick is \" +str(a_len))\n",
    "print(\"The length of words (including punctuations) in Sense & Sensibility is \" +str(b_len))\n",
    "print(\"The length of words (including punctuations) in the Book of Genesis is \" +str(c_len))\n",
    "print(\"The length of words (including punctuations) in the Inaugural Address Corpus is \" +str(d_len))\n",
    "print(\"The length of words (including punctuations) in the Chat Corpus is \" +str(e_len))\n",
    "print(\"The length of words (including punctuations) in Monty Python and the Holy Grail is \" +str(f_len))\n",
    "print(\"The length of words (including punctuations) in the Wall Street Journal is \" +str(g_len))\n",
    "print(\"The length of words (including punctuations) in the Personal Corpus is \" +str(h_len))\n",
    "print(\"The length of words (including punctuations) in The Man Who was Thursday is \" +str(i_len))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4938ea1c-ee7d-479a-b9e6-e486d29d975e",
   "metadata": {},
   "source": [
    "Now let's calculate the normalized frequency of the word \"God\" in our first book, Moby Dick \n",
    "In the code below, what we are essentially telling Python is to divide the number of times the word \"God\" appears in Moby Dick by the total number of words in Moby Dick, and then multiply that by a million and round it off to the nearest whole number and finally save the result in a variable called n_a (short for normalized frequency of a) and then print it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f336d0-b40a-4d2d-b6b3-64dc701e1e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_a = round(((a/a_len)*1000000))\n",
    "print(n_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904dfeab-0892-42ff-9f05-fb361d44d07b",
   "metadata": {},
   "source": [
    "Just like this, we can now save normalized frequencies of the word \"God\" in all our texts: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6b3beb-0e60-47a8-b4b1-069151750753",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_a = round((a/a_len)*1000000)\n",
    "n_b = round((b/b_len)*1000000)\n",
    "n_c = round((c/c_len)*1000000)\n",
    "n_d = round((d/d_len)*1000000)\n",
    "n_e = round((e/e_len)*1000000)\n",
    "n_f = round((f/f_len)*1000000)\n",
    "n_g = round((g/g_len)*1000000)\n",
    "n_h = round((h/h_len)*1000000)\n",
    "n_i = round((i/i_len)*1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdc205c-4b11-4932-b4cc-3e27f232f954",
   "metadata": {},
   "source": [
    "We can also print these like we did earlier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce51525a-4145-4a98-9fc6-40712c4cf157",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The normalized frequency of the word God in Moby Dick is \" +str(n_a))\n",
    "print(\"The normalized frequency of the word God in Sense and Sensibility is \" +str(n_b))\n",
    "print(\"The normalized frequency of the word God in the Book of Genesis is \" +str(n_c))\n",
    "print(\"The normalized frequency of the word God in the Inaugural Address Corpus is \" +str(n_d))\n",
    "print(\"The normalized frequency of the word God in the Chat Corpus is \" +str(n_e))\n",
    "print(\"The normalized frequency of the word God in Monty Python and the Holy Grail is \" +str(n_f))\n",
    "print(\"The normalized frequency of the word God in the Wall Street Journal is \" +str(n_g))\n",
    "print(\"The normalized frequency of the word God in Personal Corpus is \" +str(n_h))\n",
    "print(\"The normalized frequency of the word God in The Man Who was Thursday is \" +str(n_a))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368afb88-e223-4443-b72b-77d9321f1e6a",
   "metadata": {},
   "source": [
    "<h5> 2.1.2.2: Visualizing normalized frequencies using bar graphs </h5>\n",
    "\n",
    "Let's visualize these results now to make them readable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1df0433-f85e-49ac-9823-c6570a63d924",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "texts = ['Moby', 'Sense', 'Genesis', 'Inaugural', 'Chat','Monty','Wall','Personal','Man']\n",
    "n_frequency = [n_a,n_b,n_c,n_d,n_e,n_f,n_g,n_h,n_i]\n",
    "ax.bar(texts,n_frequency)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123a478b-d164-4a0e-a430-b5e587e22b25",
   "metadata": {},
   "source": [
    "<h5> 2.1.2.3: What has changed about our results? </h5>\n",
    "\n",
    "<b> How does this change the inferences we had made earlier? Keep this inference in mind </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe4ff14-04a3-4d3c-b2ab-8c20e863c5a0",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "       \n",
    "<h4> 2.1.3: Dispersion plots </h4>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6a07ae-e16a-431f-b92d-e82f90ea27da",
   "metadata": {},
   "source": [
    "<h5> 2.1.3.1 What are dispersion plots? </h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a9fe2f-f118-49c5-b266-9c5f4ab05f90",
   "metadata": {},
   "source": [
    "\"It is one thing to automatically detect that a particular word occurs in a text, and to display some words that appear in the same context. However, we can also determine the location of a word in the text: how many words from the beginning it appears. This positional information can be displayed using a dispersion plot. Each stripe represents an instance of a word, and each row represents the entire text\" (Bird et al., 2019)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327d4eb8-1df3-46c0-b7b5-3b9602a90a11",
   "metadata": {},
   "source": [
    "<h5> 2.1.3.2 How can we create a dispersion plot? </h5>\n",
    "\n",
    "Dispersion plots work especially well if you are trying to find some insights about how frequencies of a word or set of words has changed over time. Let's use the striking example that Bird et al. (2019) give in their book. They use text4 which contains inaugural addresses given by american presidents over the years arranged in a chronological order (i.e. the first president's speech is present at the beginning of the book and most recent presidents' speech is present at the end of the book). Let's see what their example shows us.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dcfadf-3f42-4f2b-93df-7675c7ec72ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "text4.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duties\", \"America\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb2d3ff-a1b4-4950-b983-0171590acd48",
   "metadata": {},
   "source": [
    "This example shows how the frequency of the words \"citizens\", \"democracy\", \"freedom\", \"duties\", \"America\" varies in American presidential speeches over time. \n",
    "\n",
    "<h5> 2.1.3.3 What inferences can we draw from this? </h5>\n",
    "    \n",
    "<b>Hint:</b> What does this tell us about how Presidential priorities have been changing over the decades and how is that a reflection of historical shifts?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e392de3e-2c01-43f7-bd7a-db79d4502f37",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-info\">\n",
    "    \n",
    "Now you try. \n",
    "\n",
    "Try to find create a dispersion plot for any of the texts in our corpus that interest you. Choose words that you think might show any interesting patterns. What insights can we draw from this? \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9f9563-16ea-47fc-8ebe-cf0a67fc50af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c1d8b96-897e-45c6-bd13-1c78b1231a8b",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "       \n",
    "<h4> 2.1.4: Exploring some iconic studies </h4>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2e786b-44e1-4daf-8fa8-4b2c5b14bbb1",
   "metadata": {},
   "source": [
    "<b> Good work so far! This very simple technique of counting words that interest us and comparing them across texts can lead to very insightful results. </b>\n",
    "\n",
    "1) Example 1: Let's see a wonderful example of how a researcher did this to map out how corporate organizations did or not did not support the BLM movement on their Twitter profiles, something that could then be used to put pressure on these compabies to support the movement: https://www.kmcelwee.com/fortune-100-blm-report/site/corporate-summaries.html \n",
    "\n",
    "2) Example 2: Langer et al. (2012) map out how the frequency and diversity of terms that refer to biodiversity has been steadily declining over the last 200 years in Western literature: https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1002/pan3.10256 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd1ee84-6c21-4da3-9ca4-bdc1a5962c2c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "       \n",
    "<h3> 2.2 Technique Type: 2: Close reading (micro-analysis)</h3>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b0e1d9-ef94-4796-8e58-c78f133ee3a2",
   "metadata": {},
   "source": [
    "As humanists and social scientists, thankfully we already know that simply knowing quantative differences between the number of times a word appears in two texts, gives us some clues but does not reveal to use a more in-depth picture. This is why, when text-mining researchers usually mix macro readings or distant reading approaches with more micro, qualitative ones to get a more triangulated interpretation. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97184779-f9c1-4c85-b61b-569cf3442110",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "<h4> 2.2.1: What are \"condorance lines\"? </h4>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdec5994-fe8a-4081-a2dd-dfe41a0499e8",
   "metadata": {},
   "source": [
    "Concordance lines are simply \"a list of all the occurences of a particular search term in a corpus, presented within the context in which they occur; usually a few words to the left and right of the search term\" (Baker, 2006: 71)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4772b6c1-8491-44fa-9704-7ac7f95d0ecd",
   "metadata": {},
   "source": [
    "<h5> 2.2.1.1: Visualizing concordance lines </h5>\n",
    "\n",
    "The NLTK Library has an in-built function we can use to see concordance lines of key terms that interest us\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdb445e-30ee-4d80-ad9f-7a074615cc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "text3.concordance(\"woman\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c83fe3-d01b-48c9-907a-ce3fef12c32d",
   "metadata": {},
   "source": [
    "As you can see, this function \"concordance\" displays the word we are interested in (here \"woman\" for an example) in a text of our choice (here, text 3 or book of genesis for example) along with a small slice of the sentences in which that word appears in that text. This allows us to see the semantic context in which our word appears which can be helpful for close-reading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a99312d-9298-4a86-bf7f-093feb906a1b",
   "metadata": {},
   "source": [
    "<h5> 2.2.1.2: Using concordances to explore of \"God\" inferences </h5>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e88290b-eba0-49c8-9759-543d4cf9de89",
   "metadata": {},
   "source": [
    "<b> Remember the inference you drew earlier based on the frequencies of \"God\" in these texts. Let's now try to gain a more in-depth understanding of it using some close-reading or micro analysis techniques </b>\n",
    "\n",
    "Let's say that hypothetically, an inference you drew was that the influence of the concept of \"God\" in Western human imagination is reducing over time. While in ancient texts like the Book of Genesis, it is quite high, as come to more modern texts, its frequency starts to become low. However, in certain genres, like American presidential speeches or existential texts like Moby Dick, it is still relatively high, but in other genres like web-chats or newspaper advertisements it is almost non-existent. \n",
    "\n",
    "<b> Let's now look at some concordance lines to see what contexts the word \"God\" appears in in these texts </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1dddd8-304f-445a-83d5-e85c05b82b59",
   "metadata": {},
   "source": [
    "Let's first see how it appears in Moby Dick "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5ee051-3293-4a9b-b44d-d222311709eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1.concordance(\"God\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ca8aed-883b-4531-9713-e150ad680cbe",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-info\">\n",
    "    \n",
    "Now you try. \n",
    "\n",
    "Try to display concordance lines that contain the word \"God\" in a text of your choice from the corpus that we have been using\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1ec651-05df-4f32-adeb-4b6d2939ef55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "466e2e8e-6e96-4fbd-a375-6ca6cd489237",
   "metadata": {},
   "source": [
    "<h5> 2.2.1.3: What new inferences can you draw based on such comparison?</h5>\n",
    "\n",
    "<b>Hint:</b> Focus on the difference in the context in which the word \"God\" is used in your texts of choice. Many insights are possible. You could maybe focus on the grammatical categories of the word \"God\". Has it been used as a proper noun or a common noun? What kind of tone does the sentence have where it has been used? It is very formal and reverential. Or is it more informal and irreverant? What words (nouns, adjectives, verbs etc.) appear usually near it? What do these differences tell us about the cultures and historical moments in which these texts appear? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a4a359-bcf8-4c1e-a978-96c74d1a7f41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29657589-ce6d-4430-b356-ccb096603a20",
   "metadata": {},
   "source": [
    "***\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "       \n",
    "<h4> 2.2.2: Collocates Analysis</h4>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be75c081-3989-4fa4-8bbf-4bbb72d12027",
   "metadata": {},
   "source": [
    "<h5> 2.2.2.1 What are collocates? </h5>\n",
    "\n",
    "<p> \n",
    "\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8423d676-2b14-409d-b463-64614c17526b",
   "metadata": {},
   "source": [
    "Another very useful technique for micro-anlaysis or close-reading is called collocates analysis. \"When a word regularly appears near another word and the relationship is statistically significant in some way, then such co-occurences are referred to as collocates and the phenomena of certain words frequently occuring next to or near each other is called collocation\" (Baker, 2006: 96). This can be especially useful in discourse anlaysis when we are trying to find out how certain words or concepts are understood within a text, or within a corpus of texts that represent a particular socio-historical or cultural moment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5071dbc2-2fa2-422b-9c57-faf01e61307e",
   "metadata": {},
   "source": [
    "<h5> 2.2.2.2 Finding collocates </h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2ea2cf-1918-42a9-a017-83c25bec9d79",
   "metadata": {},
   "source": [
    "Now finding collocates is a slightly more complicated procedure as it involves some slightly complicated statistics and computer programming. But thankfully, due to the beauty of Python, we can use code that other people have written. As beginners trying to understand these methods, it is okay to not understand how these procedures are leading to the outputs we are getting, but as we go more in-depth, it is helpful to learn about the intracies of these methods to see how they impact our results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec390c13-e5ae-421f-ae22-a750345f7e1a",
   "metadata": {},
   "source": [
    "<h5> 2.2.2.3 \"God\" in Amerian Presidential Speeches </h5>\n",
    "    \n",
    "The code below finds the most significant 2 word and 3 word collocates for a word we choose in a text that we choose. Here I have tweaked this code to show us the collocates of the word \"God\" in text4 or the text that contains all the American presidents' inaugural speeches. Let's run it and see what results it produces! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98219fff-58b2-45fe-b1c1-4cb85f576078",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "\n",
    "# Ngrams with 'God' as a member\n",
    "God_filter = lambda *w: 'God' not in w\n",
    "\n",
    "\n",
    "## Bigrams\n",
    "finder = BigramCollocationFinder.from_words(text4)\n",
    "# only bigrams that appear 3+ times\n",
    "finder.apply_freq_filter(3)\n",
    "# only bigrams that contain 'God'\n",
    "finder.apply_ngram_filter(God_filter)\n",
    "# return the 10 n-grams with the highest PMI\n",
    "print(finder.nbest(bigram_measures.likelihood_ratio, 7))\n",
    "\n",
    "\n",
    "## Trigrams\n",
    "finder = TrigramCollocationFinder.from_words(text4)\n",
    "# only trigrams that appear 3+ times\n",
    "finder.apply_freq_filter(3)\n",
    "# only trigrams that contain 'God'\n",
    "finder.apply_ngram_filter(God_filter)\n",
    "# return the 10 n-grams with the highest PMI\n",
    "print(finder.nbest(trigram_measures.likelihood_ratio, 7))\n",
    "\n",
    "#credit: https://9to5answer.com/nltk-collocations-for-specific-words "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc79e6b-e16f-4307-906b-2e4ff945e48a",
   "metadata": {},
   "source": [
    "<h5> 2.2.2.4 How to read this output </h5>\n",
    "\n",
    "What we see now is a list of collocates of the word \"God\" in text1. In each bracket, you see one or more words along with the word \"God\" which are its collocates. For example, in the bracket ('God','bless'), the verb \"bless\" is a collocate or a word that appears quite frequently on the right of the noun \"God\". Similarly, in the bracket ('God', 'bless', 'America'), we see that the verb \"bless\" and the noun \"America\" appear quite frequently along with the word \"God\" in American presidential speeches. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d05b32-d4e8-4921-af89-76d9b6f504fe",
   "metadata": {},
   "source": [
    "<h5> 2.2.2.5: What inference can we draw from this? </h5>\n",
    "\n",
    "<b> Hint:</b> Think about what the frequent use of the verb \"bless\" next to the noun \"God\" tells us about how American presidents frame the concept of \"God\" disursively or rhetorically in their speeches? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17cbfd8-a4a3-4804-80cf-030a286abe4e",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-info\">\n",
    "    \n",
    "Now you try. \n",
    "\n",
    "In the code below, just change the word \"text1\" wherever it appears to any other texts in our corpus that had showed a higher normalized frequency of the word \"God\" in it (text1, text2, text4, text5, text6, text7, text9 etc.) and then run the cell. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9098293-3784-4745-ac3b-dc1a8ac1d3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "\n",
    "# Ngrams with 'God' as a member\n",
    "God_filter = lambda *w: 'God' not in w\n",
    "\n",
    "\n",
    "## Bigrams\n",
    "finder = BigramCollocationFinder.from_words(text1)\n",
    "# only bigrams that appear 3+ times\n",
    "finder.apply_freq_filter(3)\n",
    "# only bigrams that contain 'God'\n",
    "finder.apply_ngram_filter(God_filter)\n",
    "# return the 10 n-grams with the highest PMI\n",
    "print(finder.nbest(bigram_measures.likelihood_ratio, 7))\n",
    "\n",
    "\n",
    "## Trigrams\n",
    "finder = TrigramCollocationFinder.from_words(text1)\n",
    "# only trigrams that appear 3+ times\n",
    "finder.apply_freq_filter(3)\n",
    "# only trigrams that contain 'God'\n",
    "finder.apply_ngram_filter(God_filter)\n",
    "# return the 10 n-grams with the highest PMI\n",
    "print(finder.nbest(trigram_measures.likelihood_ratio, 7))\n",
    "\n",
    "#credit: Adapted from the work of https://9to5answer.com/nltk-collocations-for-specific-words "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733b41b9-fd74-489c-b2af-5ca6197446d3",
   "metadata": {},
   "source": [
    "What does this output tell you about how God has been discursively or rhetorically framed within this text? What inferences can you draw based on this? How does it compare to the results we saw in our collocates analysis of \"God\" in the American presidential speeches corpus? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddc7396-7d70-4304-92ea-ac83965d1859",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "       \n",
    "<h4> 2.2.3: Exploring some iconic studies </h4>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8828d56c-8a96-4f5d-9e13-4818fe505dc4",
   "metadata": {},
   "source": [
    "<b> Good work so far! Let's explore some inconic studies now that use these techniques </b>\n",
    "\n",
    "1) Example 1: Gabrielatos & Baker (2008) analyse a huge corpus of newspapers in the UK to see what kind of collocates they commonly use to refer to refugees and asylum seekers, and they find consistent patterns like metaphors of \"water\" used to construct refugees (e.g. \"refugees are flooding the country\") to create a cognitive frame whereby the public thinks of them as a natural disaster which is plaguing their country: https://journals.sagepub.com/doi/abs/10.1177/0075424207311247"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec944751-9d55-435a-8ac8-929a6909f0c6",
   "metadata": {},
   "source": [
    "***\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "       \n",
    "<h4> Works Cited </h4>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07d24de-c503-406d-b428-90c4a255e6c4",
   "metadata": {},
   "source": [
    "Things to read for future development in Text-mining skiils: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd44e2b2-8301-4510-a44c-b6e724c1c2ae",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <h1> Part 3: Applying to your work </h1>\n",
    "\n",
    "<p>\n",
    "\n",
    "How do you think some of these techniques might apply to your work?\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904ee42d-6c0b-4a83-ad50-dc58838be157",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<h2> Acknowledgements</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd09fc73-d324-4961-a404-6db406b8f8ea",
   "metadata": {},
   "source": [
    "1. This resource was developed as part of the Workshop Series from the Digital Scholarship and Data Science Fellowship Program funded by the University Libraries at the University of Arizona. You can learn more about this program by clicking here: https://data.library.arizona.edu/data-science/ds2f\n",
    " \n",
    "2. I'm grateful to Jeffrey Oliver and Megan Senseney for their inspiring mentorship and well-scaffolded year long training program that helped me produce this resource. I'm also thankful to Jim Martin, Yvonne Mery, Leslie Sult, and Cheryl Casey for their insightful lectures on various aspects of data science, digital pedagogy, and open educatioanl resource production. \n",
    " \n",
    "3. I'm especially thankfully to Prof. Charlie Gomez and his amazing class INFO 514: Computational Social Science at the University of Arizona. A lot of the content for this notebook, especially the introduction to Jupypter Notebooks as well as the markdown code for this notebook have been adapted, with permission, from his class notebooks.\n",
    "\n",
    "4. <other people to acknowledge RCTE's ICR requirement; TAPI, Birmingham; ISTA 130 + the amazing Github and online community of debuggers>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326dcde4-354e-4489-b396-ff495b740aa4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<h2> Works Cited</h2>\n",
    "   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f25608-5c9d-4cfc-9508-e01584fa460a",
   "metadata": {},
   "source": [
    "1. ACLS. (2006). Our Cultural Commonwealth: The report of the American Council of Learned Societies Commission on Cyberinfrastructure for the Humanities and Social Sciences. ACLS. https://www.ideals.illinois.edu/items/199 <br>\n",
    "    \n",
    "2. Evans, J. A., & Aceves, P. (2016). Machine Translation: Mining Text for Social Theory. Annual Review of Sociology, 42(1), 21–50. https://doi.org/10.1146/annurev-soc-081715-074206<br>\n",
    "    \n",
    "3. Bird, Steven, Ewan Klein, and Edward Loper (2009), Natural Language Processing with Python, O'Reilly Media.<br>\n",
    "    \n",
    "4. Baker, P. (2006). Using corpora in discourse analysis. Continuum.\n",
    "\n",
    "5. <cite the iconic studies you've listed out>\n",
    "\n",
    "6. cite the collocates code>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae83fe70-b648-419c-833e-4c1ba6e22182",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<h2> Recommendations for futher reading</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c0ece6-a50c-4881-987f-5d1a3fc8a6af",
   "metadata": {},
   "source": [
    "1. https://tedunderwood.com/2012/08/14/where-to-start-with-text-mining/\n",
    "2. https://libguides.library.arizona.edu/text-mining\n",
    "3. TAPI open access Jupyter notebooks\n",
    "4. Github notebooks \n",
    "5. Walsh’s book which is made out of jupyter \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec25394-8dbc-45e2-9cea-6c5f0885fb98",
   "metadata": {},
   "source": [
    "Shield: [![CC BY 4.0][cc-by-shield]][cc-by]\n",
    "\n",
    "This work is licensed to Anuj Gupta under a\n",
    "[Creative Commons Attribution 4.0 International License][cc-by].\n",
    "\n",
    "[![CC BY 4.0][cc-by-image]][cc-by]\n",
    "\n",
    "[cc-by]: http://creativecommons.org/licenses/by/4.0/\n",
    "[cc-by-image]: https://i.creativecommons.org/l/by/4.0/88x31.png\n",
    "[cc-by-shield]: https://img.shields.io/badge/License-CC%20BY%204.0-lightgrey.svg "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
